{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fd2d32",
   "metadata": {},
   "source": [
    "# 2.2.2 Working with Data\n",
    "\n",
    "<div style=\"background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; margin: 10px;\">\n",
    "<strong>ðŸ“‹ Workshop Contents</strong>\n",
    "<ul style=\"line-height: 1.2;\">\n",
    "<li><a href=\"#What-Youll-Learn\">What You'll Learn</a></li>\n",
    "<li><a href=\"#Prerequisites\">Prerequisites</a></li>\n",
    "<li><a href=\"#Cost-Overview\">Cost Overview</a></li>\n",
    "<li><a href=\"#Step-1-Copy-CSVs-to-S3\">Step 1: Copy CSVs to S3</a></li>\n",
    "<li><a href=\"#Step-2-Using-Query-Editor-to-Create-Tables-and-Import-Small-Amount-of-Data\">Step 2: Using Query Editor to Create Tables and Import Data</a></li>\n",
    "<li><a href=\"#Step-3-Using-Query-Editor-to-Execute-SQL-Queries\">Step 3: Using Query Editor to Execute SQL Queries</a></li>\n",
    "<li><a href=\"#Step-4-Typical-Query-Tuning-Steps\">Step 4: Typical Query Tuning Steps</a></li>\n",
    "<li><a href=\"#Next-Steps\">Next Steps</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Welcome to the second part of connecting to Aurora PostgreSQL! In this notebook, we'll create tables and load data using different methods.\n",
    "\n",
    "## What You'll Learn\n",
    "- Use Query Editor in RDS Console\n",
    "- Create tables using DDL statements\n",
    "- Load data from S3\n",
    "- Optimize query performance\n",
    "\n",
    "## Prerequisites\n",
    "- Completed [2.2.1 Basic Connectivity](2.2.1_Basic_Connectivity.ipynb)\n",
    "- S3 bucket for storing data files\n",
    "- Sample data files ready to upload\n",
    "\n",
    "## Cost Overview ðŸ’°\n",
    "\n",
    "Use [AWS Pricing Calculator](https://calculator.aws/#/) to estimate the cost for your architecture solution. The following a rough cost estimation for the resources created in this notebook.\n",
    "\n",
    "| Component | Cost (us-east-1) | Notes |\n",
    "|-----------|------------------|--------|\n",
    "| S3 Standard Storage | \\$0.023/GB-month | Small CSV files (~1MB total) |\n",
    "| S3 PUT Requests | \\$0.0005/1,000 requests | 8 CSV file uploads |\n",
    "| Data Transfer | Free | S3 to Aurora in same region |\n",
    "| Aurora Query Execution | Included by Aurora cost | Refer to [Aurora cost](../2.1_Crearting_Your_First_Aurora_Cluster/2.1.2_create_your_first_aurora_postgresql_part2.ipynb) |\n",
    "\n",
    "ðŸ’¡ **Cost Optimization Tips:**\n",
    "- CSV files are small (~1MB total) resulting in minimal S3 storage costs\n",
    "- Data transfer between S3 and Aurora in the same region is free\n",
    "- Query execution uses existing Aurora cluster capacity\n",
    "- Consider S3 Intelligent Tiering for larger datasets\n",
    "- Use S3 lifecycle policies to transition old data to cheaper storage classes\n",
    "\n",
    "> **Free Tier Benefits**: New AWS accounts get 5GB of S3 Standard storage free for 12 months, easily covering this workshop's data requirements. Check [here for up-to-date Free Tier information](https://aws.amazon.com/free/).\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "> **ðŸ’¡ Important**: [Data API cancels an operation and returns a timeout error if the operation doesn't finish processing within 45 seconds](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html#data-api-timeouts).\n",
    "\n",
    "Let's begin working with our data! ðŸš€\n",
    "\n",
    "## Step 1: Copy CSVs to S3\n",
    "\n",
    "First, download sample data for testing purpose. If the CSV files don't exist in the expected location, the script will automatically download them from the provided URL from [AWS Workshop Studio](https://catalog.us-east-1.prod.workshops.aws/workshops/bbc211ed-aba2-4a6a-a2bf-227fffd3ce99/en-US): `https://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee.s3.us-east-1.amazonaws.com/bbc211ed-aba2-4a6a-a2bf-227fffd3ce99/xanadu-app/sql.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download CSV files if they don't exist and upload to S3\n",
    "\n",
    "# Set your S3 bucket name\n",
    "S3_BUCKET=\"your-s3-bucket-name\"  # Replace with your bucket name\n",
    "\n",
    "# Check if S3 bucket exists\n",
    "if ! aws s3api head-bucket --bucket \"$S3_BUCKET\" 2>/dev/null; then\n",
    "    echo \"âŒ S3 bucket '$S3_BUCKET' does not exist or is not accessible\"\n",
    "    echo \"Please create the bucket or update the S3_BUCKET variable with a valid bucket name\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "echo \"âœ… S3 bucket '$S3_BUCKET' exists and is accessible\"\n",
    "\n",
    "# Path where CSV files should be located\n",
    "CSV_PATH=\"../../3_Building_Your_First_Serverless_Web_App_with_Aurora/rewards-app-example/lambda/sql\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "mkdir -p \"$CSV_PATH\"\n",
    "\n",
    "# Check if files exist\n",
    "if [ \"$(ls -A $CSV_PATH 2>/dev/null)\" ]; then\n",
    "    echo \"âœ“ CSV files found in $CSV_PATH\"\n",
    "else\n",
    "    echo \"CSV directory is empty, downloading files...\"\n",
    "    \n",
    "    # URL for SQL files\n",
    "    SQL_URL=\"https://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee.s3.us-east-1.amazonaws.com/bbc211ed-aba2-4a6a-a2bf-227fffd3ce99/xanadu-app/sql.zip\"\n",
    "    \n",
    "    # Download and extract\n",
    "    wget -q \"$SQL_URL\" -O /tmp/sql.zip\n",
    "    unzip -q /tmp/sql.zip -d /tmp\n",
    "    \n",
    "    # Copy SQL files to CSV_PATH\n",
    "    if [ -d \"/tmp/sql\" ]; then\n",
    "        cp -r /tmp/sql/* \"$CSV_PATH/\"\n",
    "        echo \"âœ“ Copied $(ls /tmp/sql | wc -l) SQL files to $CSV_PATH\"\n",
    "    else\n",
    "        echo \"âœ— SQL directory not found in zip\"\n",
    "    fi\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    rm -rf /tmp/sql.zip /tmp/sql\n",
    "fi\n",
    "\n",
    "# Create S3 directories\n",
    "aws s3api put-object --bucket $S3_BUCKET --key sql/ddl/\n",
    "aws s3api put-object --bucket $S3_BUCKET --key sql/data/\n",
    "\n",
    "# Upload CSV files to S3\n",
    "echo \"Uploading CSV files to S3...\"\n",
    "aws s3 cp $CSV_PATH/images.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/customers.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/catalog_items.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/catalog_images.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/points_balances.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/transactions.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/order_items.csv s3://$S3_BUCKET/sql/data/\n",
    "aws s3 cp $CSV_PATH/shopping_cart_items.csv s3://$S3_BUCKET/sql/data/\n",
    "\n",
    "echo \"âœ… All files uploaded to S3 successfully!\"\n",
    "echo \"S3 Structure:\"\n",
    "aws s3 ls --recursive s3://$S3_BUCKET/sql/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6df594",
   "metadata": {},
   "source": [
    "## Step 2: Using Query Editor to Create Tables and Import Small Amount of Data\n",
    "\n",
    "### Creating Tables and Loading Data\n",
    "\n",
    "[Amazon Aurora query editor](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/query-editor.html) lets you run SQL statements on your Amazon Aurora cluster through the AWS Management Console. We'll explore using [RDS console](https://console.aws.amazon.com/rds/home) to quickly create a few tables and load small amount of data from S3.\n",
    "\n",
    "![Creating Tables and Loading Data with Amazon Aurora query editor](../images/2.2-query-editor-run-query.gif)\n",
    "\n",
    "Follow these steps in the [RDS Console](https://console.aws.amazon.com/rds):\n",
    "\n",
    "1. Check Data API is enabled (Click **Databases** -> Click **Aurora cluster identifier** -> Scroll down to the bottom -> Find **RDS Data API** column -> Confirm **Status of Data API** is **Enabled**)\n",
    "2. Open [RDS Query Editor](https://console.aws.amazon.com/rds/home?#query-editor:)\n",
    "3. Connect to your Amazon Aurora database created by [section 2.1](../2.1_Crearting_Your_First_Aurora_Cluster/README.MD)\n",
    "    - Database instance or cluster: choose the Amazon Aurora cluster from the drop-down list\n",
    "    - Database username: Connect with a Secret Manager ARN\n",
    "    - Secrets manager ARN: enter the Secret Manager ARN associated with your Amazon Aurora cluster (Click **Databases** -> Click **Aurora cluster identifier** -> Click **Configuration** tab -> Find **Authentication** column -> Copy **Master credentials ARN**)\n",
    "    - Enter the name of the database: e.g. mylab (Click **Databases** -> Click **Aurora instance identifier** -> Click **Configuration** tab -> Find **Configuration** column -> Copy **DB name**)\n",
    "4. Execute [DDLs to create tables](./sql/xpoints_schema.sql) \n",
    "5. Execute the [SQLs to import SQL from S3](./sql/xpoints_load_data.sql). Ensure you modify 'your-s3-bucket-name' and 'your-region' in the script.\n",
    "\n",
    "> Note: [Data API cancels an operation and returns a timeout error if the operation doesn't finish processing within 45 seconds](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html#data-api-timeouts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03d183",
   "metadata": {},
   "source": [
    "## Step 3: Using Query Editor to Execute SQL Queries\n",
    "\n",
    "1. **Check Row Counts**\n",
    "```sql\n",
    "-- Copy this query to RDS Query Editor\n",
    "SELECT \n",
    "    schemaname || '.' || relname AS table_name,\n",
    "    n_live_tup AS row_count\n",
    "FROM pg_stat_user_tables\n",
    "WHERE schemaname = 'xpoints'\n",
    "ORDER BY table_name;\n",
    "```\n",
    "\n",
    "2. **Sample Data Preview**\n",
    "```sql\n",
    "-- Customers sample\n",
    "SELECT * FROM xpoints.customers LIMIT 5;\n",
    "\n",
    "-- Catalog items sample\n",
    "SELECT * FROM xpoints.catalog_items LIMIT 5;\n",
    "\n",
    "-- Transactions sample\n",
    "SELECT * FROM xpoints.transactions LIMIT 5;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a9d9c",
   "metadata": {},
   "source": [
    "## Step 4: Typical Query Tuning Steps\n",
    "\n",
    "Let's look at the typical steps to optimize OLTP query during development:\n",
    "\n",
    "**OLTP Query** stands for Real-time transaction processing. \n",
    "\n",
    "Common OLTP characteristics:\n",
    "- Quick response time (< 100ms)\n",
    "- Single-row or small batch operations\n",
    "- High concurrency\n",
    "- Transaction consistency\n",
    "\n",
    "#### Query Optimization Process:\n",
    "1. Identify slow queries\n",
    "2. Analyze execution plan\n",
    "3. Implement improvements\n",
    "4. Measure results\n",
    "\n",
    "First, let's setup connection variables for us to the connect to your Aurora cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-connection-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Setup connection variables - detect Aurora clusters with cookbook tag\n",
    "echo \"Setup connection variables...\"\n",
    "\n",
    "# Allow user to specify Aurora cluster name\n",
    "AURORA_CLUSTER=\"\"  # Set this to your Aurora cluster name where the data is loaded to, e.g. aurora-sv2-xxx-cluster\n",
    "AWS_REGION=\"\"  # Set your AWS region where your Aurora cluster resides, e.g. us-east-1\n",
    "\n",
    "# Check if AWS_REGION is available\n",
    "if [ -z \"$AWS_REGION\" ]; then\n",
    "    echo \"âŒ AWS_REGION is not set\"\n",
    "    echo \"Please set the AWS_REGION variable\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "if [ -z \"$AURORA_CLUSTER\" ]; then\n",
    "    echo \"No specific cluster provided, searching for clusters with cookbook tag...\"\n",
    "    # Find Aurora clusters with CreationSource tag and matching names - get most recent\n",
    "    AURORA_CLUSTER=$(aws rds describe-db-clusters --region $AWS_REGION --query 'DBClusters[?(contains(DBClusterIdentifier, `aurora-sv2`) || DBClusterIdentifier==`aurora-demo`) && Tags[?Key==`CreationSource` && Value==`aws-database-cookbook-v2025.8`]] | sort_by(@, &ClusterCreateTime) | [-1].DBClusterIdentifier' --output text)\n",
    "    \n",
    "    if [ -z \"$AURORA_CLUSTER\" ]; then\n",
    "        echo \"âŒ No Aurora cluster found with cookbook tag\"\n",
    "        echo \"Create an Aurora cluster following the steps in Section 2 - Creating Your First Aurora Cluster\"\n",
    "        echo \"Available clusters:\"\n",
    "        aws rds describe-db-clusters --region $AWS_REGION --query 'DBClusters[*].{Identifier:DBClusterIdentifier,Engine:Engine,Status:Status}' --output table\n",
    "        exit 0\n",
    "    fi\n",
    "else\n",
    "    echo \"Using specified Aurora cluster: $AURORA_CLUSTER\"\n",
    "fi\n",
    "\n",
    "echo \"Found Aurora cluster: $AURORA_CLUSTER\"\n",
    "\n",
    "# Get managed secret from Aurora cluster\n",
    "SECRET_ARN=$(aws rds describe-db-clusters --region $AWS_REGION --db-cluster-identifier $AURORA_CLUSTER --query 'DBClusters[0].MasterUserSecret.SecretArn' --output text 2>/dev/null)\n",
    "\n",
    "if [ \"$SECRET_ARN\" != \"None\" ] && [ ! -z \"$SECRET_ARN\" ]; then\n",
    "    SECRET_NAME=$SECRET_ARN\n",
    "    echo \"Using managed secret: $SECRET_NAME\"\n",
    "else\n",
    "    # Fallback for CloudFormation stacks - find by ClusterEndpoint containing 'aurora-sv2'\n",
    "    CF_STACK_NAME=\"\"\n",
    "    for stack in $(aws cloudformation list-stacks --region $AWS_REGION --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query 'StackSummaries[*].StackName' --output text); do\n",
    "        CLUSTER_ENDPOINT=$(aws cloudformation describe-stacks --region $AWS_REGION --stack-name $stack --query \"Stacks[0].Outputs[?OutputKey=='ClusterEndpoint'].OutputValue\" --output text 2>/dev/null)\n",
    "        if [ ! -z \"$CLUSTER_ENDPOINT\" ] && [ \"$CLUSTER_ENDPOINT\" != \"None\" ] && [[ \"$CLUSTER_ENDPOINT\" == *\"aurora-sv2\"* ]]; then\n",
    "            CF_STACK_NAME=$stack\n",
    "            SECRET_NAME=$(aws cloudformation describe-stacks --region $AWS_REGION --stack-name $stack --query \"Stacks[0].Outputs[?OutputKey=='SecretArn'].OutputValue\" --output text)\n",
    "            AURORA_CLUSTER=$(echo $CLUSTER_ENDPOINT | cut -d'.' -f1)\n",
    "            echo \"Using CloudFormation secret: $SECRET_NAME\"\n",
    "            echo \"Found Aurora cluster from CloudFormation: $AURORA_CLUSTER\"\n",
    "            break\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    if [ -z \"$CF_STACK_NAME\" ]; then\n",
    "        echo \"âŒ No managed secret found\"\n",
    "        exit 1\n",
    "    fi\n",
    "fi\n",
    "\n",
    "# Get cluster endpoint\n",
    "CLUSTER_ENDPOINT=$(aws rds describe-db-clusters --region $AWS_REGION --db-cluster-identifier $AURORA_CLUSTER --query 'DBClusters[0].Endpoint' --output text)\n",
    "\n",
    "# Save connection info (no password stored)\n",
    "cat > .db_vars << EOF\n",
    "export AURORA_CLUSTER=$AURORA_CLUSTER\n",
    "export CLUSTER_ENDPOINT=$CLUSTER_ENDPOINT\n",
    "export SECRET_NAME=$SECRET_NAME\n",
    "export AWS_REGION=$AWS_REGION\n",
    "EOF\n",
    "\n",
    "echo \"âœ… Database connection variables set\"\n",
    "echo \"Cluster: $AURORA_CLUSTER\"\n",
    "echo \"Endpoint: $CLUSTER_ENDPOINT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3752b",
   "metadata": {},
   "source": [
    "Now, let's write a query that handles a customer's loyalty points redemption transaction, where a customer named 'nikki_wolf_15@example.ca' is redeeming 1,000 points to purchase a product. The system first records this redemption in the transaction history and then updates their points balance by subtracting 1,000 points from their account, ensuring both actions happen together to maintain accurate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d494cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Before Tuning - Analyze query performance\n",
    "source .db_vars\n",
    "\n",
    "echo \"Testing query performance before optimization...\"\n",
    "\n",
    "# Get credentials from Secrets Manager\n",
    "SECRET_VALUE=$(aws secretsmanager get-secret-value --region $AWS_REGION --secret-id $SECRET_NAME --query 'SecretString' --output text)\n",
    "DB_USERNAME=$(echo $SECRET_VALUE | jq -r '.username')\n",
    "DB_PASSWORD=$(echo $SECRET_VALUE | jq -r '.password')\n",
    "\n",
    "# Connect using psql with retrieved credentials\n",
    "psql \"host=$CLUSTER_ENDPOINT port=5432 dbname=mylab user=$DB_USERNAME password=$DB_PASSWORD sslmode=require\" << 'EOF'\n",
    "-- Before Tuning\n",
    "BEGIN;\n",
    "EXPLAIN (ANALYZE, VERBOSE, BUFFERS)\n",
    "WITH inserted_tx AS (\n",
    "    INSERT INTO xpoints.transactions (customer_id, tx_type, points, tx_description)\n",
    "    SELECT id, 'REDEEM', 1000, 'Product purchase'\n",
    "    FROM xpoints.customers \n",
    "    WHERE username = 'nikki_wolf_15@example.ca'\n",
    "    RETURNING customer_id\n",
    ")\n",
    "UPDATE xpoints.points_balances pb\n",
    "SET points_balance = points_balance - 1000\n",
    "FROM xpoints.customers\n",
    "WHERE pb.customer_id = customers.id\n",
    "AND customers.username = 'nikki_wolf_15@example.ca';\n",
    "ROLLBACK;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain-before-tuning",
   "metadata": {},
   "source": [
    "The above query uses [**Common Table Expressions (CTE)**](https://www.postgresql.org/docs/current/queries-with.html) features:\n",
    "\n",
    "**What are CTEs?** CTEs are temporary named result sets defined with the `WITH` clause that exist only during query execution. They make complex queries more readable and can be referenced multiple times.\n",
    "\n",
    "1. **INSERT CTE with RETURNING**: `inserted_tx AS (INSERT ... RETURNING customer_id)` inserts a transaction and returns the customer_id\n",
    "2. **UPDATE with JOIN**: Updates points_balances by joining with customers table using the same username filter\n",
    "3. **EXPLAIN ANALYZE**: Shows execution plan and actual runtime statistics\n",
    "\n",
    "This approach still has issues:\n",
    "- Two separate customer lookups (INSERT uses one, UPDATE uses another)\n",
    "- Sequential scan of customers table (no index on username)\n",
    "- The RETURNING clause from INSERT CTE is not used in the UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dc0a9",
   "metadata": {},
   "source": [
    "> **Issues Identified:**\n",
    "> 1. Multiple subqueries causing repeated table scans\n",
    "> 2. Missing index on username\n",
    "> 3. Separate statements increasing latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create necessary index\n",
    "source .db_vars\n",
    "\n",
    "echo \"Creating index for optimization...\"\n",
    "\n",
    "# Get credentials from Secrets Manager\n",
    "SECRET_VALUE=$(aws secretsmanager get-secret-value --secret-id $SECRET_NAME --query 'SecretString' --output text)\n",
    "DB_USERNAME=$(echo $SECRET_VALUE | jq -r '.username')\n",
    "DB_PASSWORD=$(echo $SECRET_VALUE | jq -r '.password')\n",
    "\n",
    "psql \"host=$CLUSTER_ENDPOINT port=5432 dbname=mylab user=$DB_USERNAME password=$DB_PASSWORD sslmode=require\" \\\n",
    "    -c \"CREATE INDEX IF NOT EXISTS idx_customers_username ON xpoints.customers(username);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimized-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# After Tuning - Test optimized query performance\n",
    "source .db_vars\n",
    "\n",
    "echo \"Testing optimized query performance...\"\n",
    "\n",
    "# Get credentials from Secrets Manager\n",
    "SECRET_VALUE=$(aws secretsmanager get-secret-value --region $AWS_REGION --secret-id $SECRET_NAME --query 'SecretString' --output text)\n",
    "DB_USERNAME=$(echo $SECRET_VALUE | jq -r '.username')\n",
    "DB_PASSWORD=$(echo $SECRET_VALUE | jq -r '.password')\n",
    "\n",
    "# Connect using psql with retrieved credentials\n",
    "psql \"host=$CLUSTER_ENDPOINT port=5432 dbname=mylab user=$DB_USERNAME password=$DB_PASSWORD sslmode=require\" << 'EOF'\n",
    "-- After Tuning\n",
    "-- Optimized query\n",
    "BEGIN;\n",
    "EXPLAIN (ANALYZE, VERBOSE, BUFFERS)\n",
    "WITH customer_data AS MATERIALIZED (\n",
    "    SELECT id FROM xpoints.customers WHERE username = 'nikki_wolf_15@example.ca'\n",
    "),\n",
    "inserted_tx AS (\n",
    "    INSERT INTO xpoints.transactions (customer_id, tx_type, points, tx_description)\n",
    "    SELECT id, 'REDEEM', 1000, 'Product purchase'\n",
    "    FROM customer_data\n",
    "    RETURNING customer_id\n",
    ")\n",
    "-- Update points balance\n",
    "UPDATE xpoints.points_balances pb\n",
    "SET points_balance = points_balance - 1000\n",
    "FROM customer_data\n",
    "WHERE pb.customer_id = customer_data.id;\n",
    "ROLLBACK;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain-after-tuning",
   "metadata": {},
   "source": [
    "The optimized query uses advanced PostgreSQL features for better performance:\n",
    "\n",
    "1. **MATERIALIZED CTE**: `customer_data AS MATERIALIZED` performs the customer lookup once and reuses the result\n",
    "2. **INSERT with CTE**: Uses the materialized customer data instead of a subquery\n",
    "3. **UPDATE with JOIN**: Updates points balance by joining with the CTE, avoiding another subquery\n",
    "4. **Index Usage**: The previously created index on username enables fast customer lookup\n",
    "\n",
    "This approach eliminates duplicate work and reduces the number of table scans from 3 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28bfe4",
   "metadata": {},
   "source": [
    "> **Improvements Made:**\n",
    "> 1. Added index on frequently queried column\n",
    "> 2. Used CTE to avoid repeated lookups\n",
    "> 3. Combined operations using CTEs\n",
    "> 4. Added MATERIALIZED hint for CTE reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c7428",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've created tables and loaded data, proceed to [Advanced Connection Management](2.2.3_Advanced_Connection_Management.ipynb) to learn how to:\n",
    "- Implement IAM authentication\n",
    "- Use AWS Secrets Manager for credentials\n",
    "- Set up RDS Proxy for connection pooling\n",
    "- Monitor and optimize database connections\n",
    "\n",
    "## Additional Resources ðŸ“š\n",
    "\n",
    "### Aurora & PostgreSQL\n",
    "- [Aurora Overview](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html)\n",
    "- [AWS Database Blog](https://aws.amazon.com/blogs/database/)\n",
    "- [PostgreSQL Documentation](https://www.postgresql.org/docs/)\n",
    "\n",
    "### Performance Tuning\n",
    "- [Aurora Best Practices](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.BestPractices.html)\n",
    "- [PostgreSQL Query Performance Tuning](https://www.postgresql.org/docs/current/performance-tips.html)\n",
    "- [Performance Insights](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PerfInsights.html)\n",
    "- [PostgreSQL Performance Optimization](https://wiki.postgresql.org/wiki/Performance_Optimization)\n",
    "\n",
    "### Data Loading & S3 Integration\n",
    "- [Aurora S3 Integration](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/postgresql-s3-import.html)\n",
    "- [Query Editor Documentation](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/query-editor.html)\n",
    "\n",
    "### Monitoring Tools\n",
    "- [pg_stat_statements](https://www.postgresql.org/docs/current/pgstatstatements.html) - Query performance statistics\n",
    "- [auto_explain](https://www.postgresql.org/docs/current/auto-explain.html) - Automatic plan logging\n",
    "- [pgBadger](https://github.com/darold/pgbadger) - Log analysis tool"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
