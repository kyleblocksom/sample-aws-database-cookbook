{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL to DocumentDB Migration Strategies\n",
    "\n",
    "<div style=\"background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 10px; margin: 10px;\">\n",
    "<strong>ðŸ“‹ Workshop Contents</strong>\n",
    "<ul style=\"line-height: 1.2;\">\n",
    "<li><a href=\"#Overview\">Overview</a></li>\n",
    "<li><a href=\"#Migration-Patterns\">Migration Patterns</a></li>\n",
    "<li><a href=\"#Data-Model-Differences-RDBMS-to-Document-Database\">Data Model Differences</a></li>\n",
    "<li><a href=\"#Migration-Options\">Migration Options</a></li>\n",
    "<li><a href=\"#Best-Practices\">Best Practices</a></li>\n",
    "<li><a href=\\\"#Validation-and-Testing\\\">Validation and Testing</a></li>\n",
    "<li><a href=\\\"#Performance-Comparison\\\">Performance Comparison</a></li>\n",
    "<li><a href=\\\"#Conclusion\\\">Conclusion</a></li>\n",
    "<li><a href=\\\"#Next-Steps\\\">Next Steps</a></li>\n",
    "<li><a href=\\\"#Additional-Resources\\\">Additional Resources ðŸ“š</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "## Overview\n",
    "This guide demonstrates different approaches to migrate data from PostgreSQL to Amazon DocumentDB, focusing on handling JSON data efficiently. Following our previous example ([Understanding the JSON in RDBMS Anti-Pattern](../7.2_Understanding-the-JSON-in-RDBMS-Antipattern/README.md)), where we identified performance limitations with JSON data storage in PostgreSQL, we'll explore migration strategies to DocumentDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Consider Migration?\n",
    "\n",
    "In our previous module, we encountered several challenges with PostgreSQL JSON storage:\n",
    "- Slow query performance on complex JSON queries\n",
    "- Limited indexing capabilities for nested JSON structures\n",
    "- Increasing query times with growing data volume\n",
    "\n",
    "Amazon DocumentDB offers several advantages for JSON workloads:\n",
    "- Native JSON document storage\n",
    "- Better query performance for nested JSON structures\n",
    "- Horizontal scaling capabilities\n",
    "- Purpose-built for JSON document workflows\n",
    "\n",
    "> ðŸ’¡ **Key Consideration:** Before proceeding with migration, evaluate your specific use case. While DocumentDB excels at JSON document storage, it might not be the best choice for all scenarios, especially if your workload involves complex joins or traditional relational data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- âœ… Access to source PostgreSQL database\n",
    "- âœ… Amazon DocumentDB cluster\n",
    "- âœ… Appropriate network connectivity\n",
    "- âœ… Required permissions on both systems\n",
    "- âœ… **Jupyter Notebook**: You can launch a [free tier Amazon SageMaker Jupyter Notebook](../../1_Getting_Started_with_AWS/1.4_Setting_up_Your_Cookbook_Environment/README.MD)\n",
    "\n",
    "### What We'll Cover\n",
    "1. Different migration patterns and when to use them\n",
    "2. Implementation approaches (AWS DMS and custom Python script)\n",
    "3. Best practices and performance considerations\n",
    "4. Post-migration validation and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Patterns\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Full Load</th>\n",
    "        <th>Full Load + CDC</th>\n",
    "        <th>CDC Only</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>One-time complete data copy</td>\n",
    "        <td>Initial data copy followed by continuous synchronization</td>\n",
    "        <td>Captures and replicates only data changes</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"3\" align=\"center\"><strong>Best For:</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            â€¢ Initial migrations<br>\n",
    "            â€¢ Small to medium datasets<br>\n",
    "            â€¢ When downtime is acceptable\n",
    "        </td>\n",
    "        <td>\n",
    "            â€¢ Production systems<br>\n",
    "            â€¢ Minimal downtime requirements<br>\n",
    "            â€¢ Large datasets\n",
    "        </td>\n",
    "        <td>\n",
    "            â€¢ Systems with existing data sync<br>\n",
    "            â€¢ Continuous replication needs<br>\n",
    "            â€¢ Real-time data synchronization\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model Differences: RDBMS to Document Database\n",
    "\n",
    "When migrating from PostgreSQL (RDBMS) to DocumentDB (Document Database), we're not just moving data - we're transforming how it's stored and accessed:\n",
    "\n",
    "## Data Model Comparison\n",
    "\n",
    "| PostgreSQL (Source) | DocumentDB (Target) |\n",
    "|-------------------|-------------------|\n",
    "| Data normalized across multiple tables | Data stored as self-contained documents |\n",
    "| Relationships maintained through foreign keys | Nested structures replace table relationships |\n",
    "| JSON stored as a column type | Native JSON support with optimized querying |\n",
    "| Schema is strictly defined | Flexible, schema-less design |\n",
    "| Complex joins needed for data retrieval | No joins needed - data embedded in documents |\n",
    "\n",
    "\n",
    "### Key Benefits\n",
    "- Improved query performance on JSON data\n",
    "- Better support for nested data structures\n",
    "- Simplified data model without joins\n",
    "- Flexible schema for evolving data structures\n",
    "- Native indexing for JSON fields\n",
    "\n",
    "This transformation allows for more efficient handling of document-oriented workloads while eliminating the overhead of JSON parsing and complex joins in PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pg-document.png](../images/7.2-postgresql-document-storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Options\n",
    "\n",
    "### Option 1: AWS Database Migration Service (DMS)\n",
    "\n",
    "AWS DMS is a fully managed service that enables you to migrate databases to AWS quickly and securely. It supports homogeneous migrations (like PostgreSQL to PostgreSQL) and heterogeneous migrations (like PostgreSQL to DocumentDB).\n",
    "\n",
    "#### How AWS DMS Works\n",
    "- Creates a replication instance that performs the migration\n",
    "- Reads data from source database\n",
    "- Formats the data for the target database\n",
    "- Loads the data into the target database\n",
    "- Maintains data consistency through CDC if configured\n",
    "- Validates data migration completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-Level Migration Steps\n",
    "![ERD-Page-12.jpg](../images/7.1-database-selection-erd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Custom Python Migration Script\n",
    "\n",
    "Below is a Python implementation for data migration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PostgreSQLToDocumentDBMigrator:\n",
    "    def __init__(self, pg_config: Dict, docdb_config: Dict):\n",
    "        self.pg_config = pg_config\n",
    "        self.docdb_config = docdb_config\n",
    "        self.pg_conn = None\n",
    "        self.docdb_client = None\n",
    "        \n",
    "    def connect_postgresql(self):\n",
    "        \"\"\"Establish connection to PostgreSQL\"\"\"\n",
    "        try:\n",
    "            self.pg_conn = psycopg2.connect(**self.pg_config)\n",
    "            logger.info(\"Connected to PostgreSQL successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to PostgreSQL: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def connect_documentdb(self):\n",
    "        \"\"\"Establish connection to DocumentDB\"\"\"\n",
    "        try:\n",
    "            connection_string = f\"mongodb://{self.docdb_config['username']}:{self.docdb_config['password']}@{self.docdb_config['host']}:{self.docdb_config['port']}/{self.docdb_config['database']}?ssl=true&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false\"\n",
    "            self.docdb_client = MongoClient(connection_string)\n",
    "            # Test connection\n",
    "            self.docdb_client.admin.command('ping')\n",
    "            logger.info(\"Connected to DocumentDB successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to DocumentDB: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_data_from_postgresql(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Extract data from PostgreSQL using provided query\"\"\"\n",
    "        try:\n",
    "            cursor = self.pg_conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            \n",
    "            # Get column names\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            \n",
    "            # Fetch all rows and convert to dictionaries\n",
    "            rows = cursor.fetchall()\n",
    "            data = [dict(zip(columns, row)) for row in rows]\n",
    "            \n",
    "            logger.info(f\"Extracted {len(data)} records from PostgreSQL\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to extract data from PostgreSQL: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            cursor.close()\n",
    "    \n",
    "    def transform_data(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Transform PostgreSQL data for DocumentDB\"\"\"\n",
    "        transformed_data = []\n",
    "        \n",
    "        for record in data:\n",
    "            # Convert datetime objects to strings\n",
    "            for key, value in record.items():\n",
    "                if isinstance(value, datetime):\n",
    "                    record[key] = value.isoformat()\n",
    "                elif isinstance(value, str) and self._is_json_string(value):\n",
    "                    # Parse JSON strings\n",
    "                    try:\n",
    "                        record[key] = json.loads(value)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Keep as string if not valid JSON\n",
    "                        pass\n",
    "            \n",
    "            transformed_data.append(record)\n",
    "        \n",
    "        logger.info(f\"Transformed {len(transformed_data)} records\")\n",
    "        return transformed_data\n",
    "    \n",
    "    def _is_json_string(self, value: str) -> bool:\n",
    "        \"\"\"Check if string is a valid JSON\"\"\"\n",
    "        try:\n",
    "            json.loads(value)\n",
    "            return True\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    def load_data_to_documentdb(self, data: List[Dict], collection_name: str):\n",
    "        \"\"\"Load transformed data into DocumentDB\"\"\"\n",
    "        try:\n",
    "            db = self.docdb_client[self.docdb_config['database']]\n",
    "            collection = db[collection_name]\n",
    "            \n",
    "            if data:\n",
    "                result = collection.insert_many(data)\n",
    "                logger.info(f\"Inserted {len(result.inserted_ids)} documents into {collection_name}\")\n",
    "            else:\n",
    "                logger.warning(\"No data to insert\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data to DocumentDB: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def migrate_table(self, table_name: str, collection_name: str = None, custom_query: str = None):\n",
    "        \"\"\"Migrate a single table from PostgreSQL to DocumentDB\"\"\"\n",
    "        if collection_name is None:\n",
    "            collection_name = table_name\n",
    "        \n",
    "        if custom_query is None:\n",
    "            query = f\"SELECT * FROM {table_name}\"\n",
    "        else:\n",
    "            query = custom_query\n",
    "        \n",
    "        logger.info(f\"Starting migration of {table_name} to {collection_name}\")\n",
    "        \n",
    "        # Extract, Transform, Load\n",
    "        data = self.extract_data_from_postgresql(query)\n",
    "        transformed_data = self.transform_data(data)\n",
    "        self.load_data_to_documentdb(transformed_data, collection_name)\n",
    "        \n",
    "        logger.info(f\"Completed migration of {table_name}\")\n",
    "    \n",
    "    def close_connections(self):\n",
    "        \"\"\"Close database connections\"\"\"\n",
    "        if self.pg_conn:\n",
    "            self.pg_conn.close()\n",
    "            logger.info(\"PostgreSQL connection closed\")\n",
    "        \n",
    "        if self.docdb_client:\n",
    "            self.docdb_client.close()\n",
    "            logger.info(\"DocumentDB connection closed\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    pg_config = {\n",
    "        'host': 'your-postgresql-host',\n",
    "        'database': 'your-database',\n",
    "        'user': 'your-username',\n",
    "        'password': 'your-password',\n",
    "        'port': 5432\n",
    "    }\n",
    "    \n",
    "    docdb_config = {\n",
    "        'host': 'your-documentdb-cluster-endpoint',\n",
    "        'database': 'your-database',\n",
    "        'username': 'your-username',\n",
    "        'password': 'your-password',\n",
    "        'port': 27017\n",
    "    }\n",
    "    \n",
    "    # Initialize migrator\n",
    "    migrator = PostgreSQLToDocumentDBMigrator(pg_config, docdb_config)\n",
    "    \n",
    "    try:\n",
    "        # Connect to both databases\n",
    "        migrator.connect_postgresql()\n",
    "        migrator.connect_documentdb()\n",
    "        \n",
    "        # Migrate specific tables\n",
    "        migrator.migrate_table('user_profiles', 'users')\n",
    "        migrator.migrate_table('product_catalog', 'products')\n",
    "        \n",
    "        # Custom query example for complex transformations\n",
    "        custom_query = \"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            name,\n",
    "            email,\n",
    "            preferences::text as preferences,\n",
    "            created_at\n",
    "        FROM user_profiles \n",
    "        WHERE active = true\n",
    "        \"\"\"\n",
    "        migrator.migrate_table('user_profiles', 'active_users', custom_query)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Migration failed: {e}\")\n",
    "    finally:\n",
    "        migrator.close_connections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Pre-Migration Planning\n",
    "- **Data Assessment**: Analyze your PostgreSQL schema and identify JSON columns\n",
    "- **Performance Baseline**: Establish current query performance metrics\n",
    "- **Schema Design**: Plan your DocumentDB collection structure\n",
    "- **Index Strategy**: Identify required indexes for DocumentDB\n",
    "\n",
    "### 2. Migration Execution\n",
    "- **Batch Processing**: Process data in manageable chunks\n",
    "- **Error Handling**: Implement robust error handling and retry logic\n",
    "- **Progress Monitoring**: Track migration progress and performance\n",
    "- **Data Validation**: Verify data integrity throughout the process\n",
    "\n",
    "### 3. Post-Migration Optimization\n",
    "- **Index Creation**: Create appropriate indexes based on query patterns\n",
    "- **Query Optimization**: Optimize queries for DocumentDB\n",
    "- **Performance Testing**: Compare performance with PostgreSQL baseline\n",
    "- **Monitoring Setup**: Implement comprehensive monitoring\n",
    "\n",
    "### 4. Common Pitfalls to Avoid\n",
    "- **Over-normalization**: Don't replicate relational patterns in DocumentDB\n",
    "- **Missing Indexes**: Ensure proper indexing for query performance\n",
    "- **Large Documents**: Avoid documents larger than 16MB\n",
    "- **Inefficient Queries**: Optimize queries for document structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Testing\n",
    "\n",
    "After migration, it's crucial to validate that the data has been transferred correctly and that the new system performs as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_migration(migrator, pg_table: str, docdb_collection: str):\n",
    "    \"\"\"Validate migration by comparing record counts and sample data\"\"\"\n",
    "    \n",
    "    # Count records in PostgreSQL\n",
    "    pg_cursor = migrator.pg_conn.cursor()\n",
    "    pg_cursor.execute(f\"SELECT COUNT(*) FROM {pg_table}\")\n",
    "    pg_count = pg_cursor.fetchone()[0]\n",
    "    \n",
    "    # Count documents in DocumentDB\n",
    "    db = migrator.docdb_client[migrator.docdb_config['database']]\n",
    "    collection = db[docdb_collection]\n",
    "    docdb_count = collection.count_documents({})\n",
    "    \n",
    "    logger.info(f\"PostgreSQL {pg_table}: {pg_count} records\")\n",
    "    logger.info(f\"DocumentDB {docdb_collection}: {docdb_count} documents\")\n",
    "    \n",
    "    if pg_count == docdb_count:\n",
    "        logger.info(\"âœ… Record counts match\")\n",
    "        return True\n",
    "    else:\n",
    "        logger.error(\"âŒ Record counts don't match\")\n",
    "        return False\n",
    "\n",
    "# Example validation\n",
    "# validate_migration(migrator, 'user_profiles', 'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare query performance between PostgreSQL and DocumentDB for JSON operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_queries():\n",
    "    \"\"\"Compare query performance between PostgreSQL and DocumentDB\"\"\"\n",
    "    \n",
    "    # PostgreSQL JSON query\n",
    "    pg_query = \"\"\"\n",
    "    SELECT * FROM user_profiles \n",
    "    WHERE preferences->>'theme' = 'dark' \n",
    "    AND preferences->'notifications'->>'email' = 'true'\n",
    "    \"\"\"\n",
    "    \n",
    "    # DocumentDB equivalent query\n",
    "    docdb_query = {\n",
    "        \"preferences.theme\": \"dark\",\n",
    "        \"preferences.notifications.email\": \"true\"\n",
    "    }\n",
    "    \n",
    "    # Benchmark PostgreSQL\n",
    "    start_time = time.time()\n",
    "    # Execute PostgreSQL query here\n",
    "    pg_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark DocumentDB\n",
    "    start_time = time.time()\n",
    "    # Execute DocumentDB query here\n",
    "    docdb_time = time.time() - start_time\n",
    "    \n",
    "    logger.info(f\"PostgreSQL query time: {pg_time:.3f}s\")\n",
    "    logger.info(f\"DocumentDB query time: {docdb_time:.3f}s\")\n",
    "    logger.info(f\"Performance improvement: {((pg_time - docdb_time) / pg_time * 100):.1f}%\")\n",
    "\n",
    "# benchmark_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Migrating from PostgreSQL to DocumentDB can provide significant benefits for JSON-heavy workloads:\n",
    "\n",
    "### Key Takeaways\n",
    "- **Performance Gains**: Native JSON support eliminates parsing overhead\n",
    "- **Simplified Queries**: No complex joins needed for nested data\n",
    "- **Better Scalability**: Horizontal scaling capabilities\n",
    "- **Flexible Schema**: Easier to evolve data structures\n",
    "\n",
    "### When to Consider Migration\n",
    "- Heavy use of JSON data types in PostgreSQL\n",
    "- Performance issues with JSON queries\n",
    "- Need for horizontal scaling\n",
    "- Document-oriented data access patterns\n",
    "\n",
    "### Migration Action Plan\n",
    "1. Assess your current PostgreSQL JSON usage\n",
    "2. Plan your DocumentDB schema design\n",
    "3. Set up a test migration environment\n",
    "4. Validate performance improvements\n",
    "5. Plan your production migration strategy\n",
    "\n",
    "Remember that migration is not always the answer - evaluate your specific use case and requirements before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources ðŸ“š\n",
    "\n",
    "### PostgreSQL JSON Features\n",
    "- [PostgreSQL JSON Types](https://www.postgresql.org/docs/current/datatype-json.html)\n",
    "- [JSON Functions and Operators](https://www.postgresql.org/docs/current/functions-json.html)\n",
    "- [GIN Indexes for JSON](https://www.postgresql.org/docs/current/gin-intro.html)\n",
    "\n",
    "### Purpose-Built Databases\n",
    "- [Amazon DocumentDB](https://docs.aws.amazon.com/documentdb/)\n",
    "- [Amazon DynamoDB](https://docs.aws.amazon.com/dynamodb/)\n",
    "- [Database Selection Guide](https://aws.amazon.com/products/databases/)\n",
    "\n",
    "### Performance & Migration\n",
    "- [Database Migration Best Practices](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html)\n",
    "- [Performance Tuning PostgreSQL](https://wiki.postgresql.org/wiki/Performance_Optimization)\n",
    "- [AWS Database Migration Service](https://docs.aws.amazon.com/dms/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
